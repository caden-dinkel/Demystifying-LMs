To first get a computer to understand our natural language, we must break the language down into a form understandable by computers. This process is called tokenization. The process of tokenization is analogous to a child learning English with phonix. First, a child might learn the root words, before moving on to conjuction and adding suffixes and/or prefixes. Much like this child learning English, the computer breaks each word into subcomponents, but instead of splitting words based on roots, suffixes, and prefixes, the words are split depending on the frequency at which chains of letters appear in the data in question.
There are many methods by which this can be done, all of which are a topic for another website. In general though it may look something like this...
