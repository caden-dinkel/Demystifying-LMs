When using large language models (LLMs), it’s easy to think that they understand language like we do. However, it's very much not true. Language models, at their core, do one action. They repeatedly predict the next most likely word or “token”. Now, this may seem a little weird, but it's the core of how AI works.
This simple process, when scaled up, becomes incredibly powerful. The model takes your input, which it breaks into tokens. These can be full words or parts of them. The tokens then are processed with highly complex math, this is the ‘model’ itself, its the part that is trained. Next a list of all possible next tokens is generated, this is done by taking the context of each token and creating a probability spread. Finally the model selects one of the tokens from the previous step. This loop is repeated, adding the new token to the context each time, eventually generating full sentences, paragraphs, and more. This can be simplified into four steps, Tokenization, Model, Probability Spread, and Selection.
This core ability is what allows the model to be used in a huge variety of applications, including advanced planners, text summarizers, chatbots, customer service agents, and much more.
